{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Using_multible_GPUs_One_Device_Strategy_Messy_vs_Clean_Room",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Glitch0000/Using_multible_GPUs_One_Device_Strategy_Messy_vs_Clean_Room/blob/main/Using_multible_GPUs_One_Device_Strategy_Messy_vs_Clean_Room.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX3nSLVSs_hK"
      },
      "source": [
        "# One Device Strategy an example to classify Messy vs Clean Room\n",
        "\n",
        "In this example we will use [One Device Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy). This is typically used to deliberately test the code on a single device. This can be used before switching to a different strategy that distributes across multiple devices.\n",
        "\n",
        " The dataset used is [Messy vs Clean Room](https://www.kaggle.com/cdawn1/messy-vs-clean-room)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXBhPqhRs_hK"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpbGH-egdhC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaaCpfzVmIPr"
      },
      "source": [
        "!unzip  /content/archive.zip #unzip data in the directory\r\n",
        "#IIn colab it is not possible to include the folders of the images before unziping them"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc5A8XXcmMoR"
      },
      "source": [
        "img_size= 420"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GajHHHqGmNMq",
        "outputId": "4995deb6-fc4f-4006-fafd-312f6bad9702"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# All images will be rescaled by 1./255\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "      rescale=1./255,\r\n",
        "      rotation_range=40,\r\n",
        "      width_shift_range=0.2,\r\n",
        "      height_shift_range=0.2,\r\n",
        "      shear_range=0.2,\r\n",
        "      zoom_range=0.2,\r\n",
        "      fill_mode='nearest',\r\n",
        "      horizontal_flip=True\r\n",
        "      )\r\n",
        "\r\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\r\n",
        "\r\n",
        "# Flow training images in batches of 128 using train_datagen generator\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "        '/content/images/images/train',  # This is the source directory for training images\r\n",
        "        target_size=(img_size, img_size),  # All images will be resized to img_size x img_size\r\n",
        "        batch_size=16,\r\n",
        "        #  class_mode='binary'in case we use binary_crossentropy loss, we need binary labels  \r\n",
        "        class_mode= 'categorical')\r\n",
        "\r\n",
        "# Flow validation images in batches of batch_size\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "        '/content/images/images/val',  # This is the source directory for validation images\r\n",
        "        target_size=(img_size, img_size),  \r\n",
        "        batch_size=8,\r\n",
        "        class_mode= 'categorical')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 192 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM--1RKAs_hM"
      },
      "source": [
        "## Define the Distribution Strategy\n",
        "\n",
        "We can list available devices in our machine and specify a device type. This allows us to verify the device name to pass in `tf.distribute.OneDeviceStrategy()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwpHZjtus_hM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d02c56f-94e4-4fff-c182-2cfb1b50dfe5"
      },
      "source": [
        "# choose a device type such as CPU or GPU\n",
        "devices = tf.config.list_physical_devices('GPU')\n",
        "print(devices[0])\n",
        "\n",
        "# the name will look something like \"/physical_device:GPU:0\"\n",
        "# Just take the GPU:0 part and use that as the name\n",
        "gpu_name = \"GPU:0\"\n",
        "\n",
        "# define the strategy and pass in the device name\n",
        "one_strategy = tf.distribute.OneDeviceStrategy(device=gpu_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVMXbPjqs_hP"
      },
      "source": [
        "## Define and Configure the Model\n",
        "\n",
        "As with other strategies, setting up the model requires minimal code changes. Let's first define a utility function to build and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx8MEwUl1300"
      },
      "source": [
        "# tells if we want to freeze the layer weights of our feature extractor during training\n",
        "do_fine_tuning = False"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XetJgxTSATdG"
      },
      "source": [
        "def build_and_compile_model():\r\n",
        "  \r\n",
        "    # defining the model\r\n",
        "    model = tf.keras.models.Sequential([\r\n",
        "    # Note the input shape is the desired size of the image img_size x img_size with 3 bytes color\r\n",
        "    # This is the first convolution\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    # The second convolution\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The third convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    # 512 neuron hidden layer\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "   \r\n",
        "    tf.keras.layers.Dense(2, activation='softmax')# number of classes, in case we have binary classification:(1, activation='sigmid')\r\n",
        "              ])\r\n",
        "\r\n",
        "    # display summary\r\n",
        "    model.summary()\r\n",
        "\r\n",
        "    # configure the optimizer, loss and metrics\r\n",
        "    optimizer = tf.keras.optimizers.SGD(lr=0.002, momentum=0.9) if do_fine_tuning else 'adam'\r\n",
        "    model.compile(optimizer=optimizer,\r\n",
        "                loss='categorical_crossentropy',#loss='sparse_categorical_crossentropy',\r\n",
        "                metrics=['accuracy'])\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnZ0626xDa-M"
      },
      "source": [
        "Note: here we faced data mismatch due to the reason of using sparse_categorical_crossentropy as a loss function instead of categorical_crossentropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnnhGHls_hQ"
      },
      "source": [
        "Now we call the function under the strategy scope. This places variables and computations on the device we specified earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDenpJX-2EhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c4ee65-4d8c-422b-d479-a9e9007e9e3b"
      },
      "source": [
        "# build and compile under the strategy scope\n",
        "with one_strategy.scope():\n",
        "    model = build_and_compile_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 418, 418, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 209, 209, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 207, 207, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 103, 103, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 101, 101, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 50, 50, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160000)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               81920512  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 81,945,122\n",
            "Trainable params: 81,945,122\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOZ6sFas_hR"
      },
      "source": [
        "`model.fit()` can be run as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L4C5KKs3fal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca388f9-1e02-4bf1-bb43-c1481c607a0e"
      },
      "source": [
        "EPOCHS = 15\n",
        "hist = model.fit(train_generator,\n",
        "                 epochs=EPOCHS,\n",
        "                 validation_data=validation_generator)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "12/12 [==============================] - 9s 745ms/step - loss: 0.5735 - accuracy: 0.7396 - val_loss: 0.5707 - val_accuracy: 0.6500\n",
            "Epoch 2/15\n",
            "12/12 [==============================] - 9s 740ms/step - loss: 0.4875 - accuracy: 0.7917 - val_loss: 0.4145 - val_accuracy: 0.8000\n",
            "Epoch 3/15\n",
            "12/12 [==============================] - 9s 732ms/step - loss: 0.4885 - accuracy: 0.7760 - val_loss: 0.3310 - val_accuracy: 0.8000\n",
            "Epoch 4/15\n",
            "12/12 [==============================] - 9s 733ms/step - loss: 0.5079 - accuracy: 0.7708 - val_loss: 0.4326 - val_accuracy: 0.8000\n",
            "Epoch 5/15\n",
            "12/12 [==============================] - 9s 738ms/step - loss: 0.4945 - accuracy: 0.7656 - val_loss: 0.4516 - val_accuracy: 0.7500\n",
            "Epoch 6/15\n",
            "12/12 [==============================] - 9s 743ms/step - loss: 0.5550 - accuracy: 0.6927 - val_loss: 0.3993 - val_accuracy: 0.9500\n",
            "Epoch 7/15\n",
            "12/12 [==============================] - 9s 736ms/step - loss: 0.5283 - accuracy: 0.7188 - val_loss: 0.5713 - val_accuracy: 0.6500\n",
            "Epoch 8/15\n",
            "12/12 [==============================] - 9s 736ms/step - loss: 0.5378 - accuracy: 0.7448 - val_loss: 0.4148 - val_accuracy: 0.8000\n",
            "Epoch 9/15\n",
            "12/12 [==============================] - 9s 737ms/step - loss: 0.4646 - accuracy: 0.7969 - val_loss: 0.3939 - val_accuracy: 0.8000\n",
            "Epoch 10/15\n",
            "12/12 [==============================] - 9s 738ms/step - loss: 0.5495 - accuracy: 0.7083 - val_loss: 0.4893 - val_accuracy: 0.7500\n",
            "Epoch 11/15\n",
            "12/12 [==============================] - 9s 735ms/step - loss: 0.5021 - accuracy: 0.7292 - val_loss: 0.4096 - val_accuracy: 0.8000\n",
            "Epoch 12/15\n",
            "12/12 [==============================] - 9s 736ms/step - loss: 0.4968 - accuracy: 0.7552 - val_loss: 0.4715 - val_accuracy: 0.7500\n",
            "Epoch 13/15\n",
            "12/12 [==============================] - 9s 737ms/step - loss: 0.5470 - accuracy: 0.7292 - val_loss: 0.5442 - val_accuracy: 0.6500\n",
            "Epoch 14/15\n",
            "12/12 [==============================] - 9s 739ms/step - loss: 0.5068 - accuracy: 0.7448 - val_loss: 0.3563 - val_accuracy: 0.8500\n",
            "Epoch 15/15\n",
            "12/12 [==============================] - 9s 737ms/step - loss: 0.4426 - accuracy: 0.8125 - val_loss: 0.2383 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ECUlE8s_hR"
      },
      "source": [
        "Since everything is working correctly, we can switch to a different device or a different strategy that distributes to multiple devices."
      ]
    }
  ]
}